knitr::opts_chunk$set(echo = TRUE)
packages.used=c("e1071","EBImage","ggplot2")
packages.needed=setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(EBImage)
library(e1071)
library(ggplot2)
experiment_dir <- "../data/raw_images/"
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=FALSE            # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =TRUE      #process features for gievn training set
new.feature.test=TRUE       # process features for independent testing set
run.test=TRUE          # run evaluation on an independent test set
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=FALSE            # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =TRUE      #process features for gievn training set
new.feature.test=TRUE       # process features for independent testing set
run.test=FALSE         # run evaluation on an independent test set
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(experiment_dir,                                                 data_name="new_feature", export=T))
#Save new features for all given data:
save(X, file="../output/Newfeature_train.RData")  #X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(experiment_dir,                                                 data_name="new_feature", export=T))
#Save new features for all given data:
save(X, file="../output/Newfeature_train.RData")  #X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(experiment_dir,                                                 data_name="new_feature", export=T))
#Save new features for all given data:
save(X, file="../output/Newfeature_train.RData")  #X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
experiment_dir <- "../data/training_data/raw_images/"
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(experiment_dir,                                                 data_name="new_feature", export=T))
#Save new features for all given data:
save(X, file="../output/Newfeature_train.RData")  #X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
length(list.files(experiment_dir))
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="new_feature", export=T))
#Save new features for all given data:
save(X, file="../output/Newfeature_train.RData")  #X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="new_feature", export=T))
#Save new features for all given data:
save(X, file="../output/Newfeature_train.RData")  #X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
tm_feature_indetest<-NA
if(new.feature.test){
tm_feature_indetest<- system.time(X_inde_test<- extract_feature(experiment_dir,                                                 data_name="new_feature", export=T))
#Save new features for the independent testing data:
save(X_inde_test, file="../output/Newfeature_inde_test.RData")
}
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
Train.y<-y[Index]
Test.x<-data.matrix(X[-Index,])
Test.y<-y[-Index]
source("../lib/Train_svm.R")
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=FALSE            # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =TRUE      #process features for gievn training set
new.feature.test=TRUE       # process features for independent testing set
run.test=FALSE         # run evaluation on an independent test set
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
best_gamma<-performace_svm$gamma[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- Train_svm(Train.x,Train.y,
gamma=best_gamma,cost = best_cost))
} else {
tm_train <- system.time(fit_train <- Train_svm(Train.x,Train.y))
}
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
best_gamma<-performace_svm$gamma[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,
gamma=best_gamma,cost = best_cost))
} else {
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y))
}
##Save the model:
save(fit_train, file="../output/fit_train.RData")
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_train==Test.y)
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
#cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
#cat("Time for making prediction=", tm_test[1], "s \n")
knitr::opts_chunk$set(echo = TRUE)
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=FALSE            # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =TRUE      #process features for gievn training set
new.feature.test=FALSE       # process features for independent testing set
run.test=FALSE         # run evaluation on an independent test set
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="Newfeature_train", export=T))
#X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
##here is where all the given 2000 pictures are put
experiment_dir <- "../data/training_data/raw_images/"
##here is where all the independent testing set are put (it will be given in class)
#testing_dir <- "../data/testing_data/"
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="Newfeature_train", export=T))
#X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
source("../lib/Train_svm.R")
Option_1<-TRUE  ##If you choose FALSE,Option 2 will be used
if (run.cv){
##Set values for the parameters:
if (Option_1){
candidates<-list(cost=c(.1,0.5,1,1.5),gamma=c(.001,0.005,0.08,.01))
} else {
candidates<-list(cost=c(.1,0.3,0.5,0.7,0.9,1.1,1.3,1.5),gamma=c(.001,0.003,0.005,0.007,.01,0.03))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="radial",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$svm_tune
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=TRUE           # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =TRUE      #process features for gievn training set
new.feature.test=FALSE       # process features for independent testing set
run.test=FALSE         # run evaluation on an independent test set
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
X<-load("../output/new_feature_.RData")
#X<-load("../output/new_feature_.RData")
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
X<-load("../output/new_feature_.RData")
dim(x)
dim(X)
X<-load("../output/new_feature.RData")
X<-load("../output/Newfeature_train.RData.RData")
X<-load("../output/Newfeature_train.RData")
dim(X)
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(img_dir=experiment_dir,                                                 data_name="Newfeature_train", export=T))
#X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
tm_feature_indetest<-NA
if(new.feature.test){
tm_feature_indetest<- system.time(X_inde_test<- extract_feature(testing_dir,                                                 data_name="Newfeature_inde_test", export=T))
}
if(set_seed){
set.seed(0)
Index<-sample(n,round(train_proportion*n,1),replace = F)
} else{
Index<-sample(n,round(train_proportion*n,1),replace = F)
}
#n is the No. of all provided data
Train.x<- data.matrix(X[Index,])
Train.y<-y[Index]
Test.x<-data.matrix(X[-Index,])
Test.y<-y[-Index]
source("../lib/Train_svm.R")
Option_1<-TRUE  ##If you choose FALSE,Option 2 will be used
if (run.cv){
##Set values for the parameters:
if (Option_1){
candidates<-list(cost=c(.1,0.5,1,1.5),gamma=c(.001,0.005,0.08,.01))
} else {
candidates<-list(cost=c(.1,0.3,0.5,0.7,0.9,1.1,1.3,1.5),gamma=c(.001,0.003,0.005,0.007,.01,0.03))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="radial",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$svm_tune
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
packages.used=c("e1071","EBImage","ggplot2")
packages.needed=setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(EBImage)
library(e1071)
library(ggplot2)
if (run.cv){
##Set values for the parameters:
if (Option_1){
candidates<-list(cost=c(.1,0.5,1,1.5),gamma=c(.001,0.005,0.08,.01))
} else {
candidates<-list(cost=c(.1,0.3,0.5,0.7,0.9,1.1,1.3,1.5),gamma=c(.001,0.003,0.005,0.007,.01,0.03))
}
##Tune the model:
tc<-tune.control(cross = K)  #set K-folds based on user-selection
svm_tune <- tune(svm, train.x=Train.x, train.y=Train.y,
kernel="radial",scale=F,ranges=candidates,tunecontrol = tc)
##Get the performacne for each model:
performace_svm<-svm_tune$svm_tune
##Save resilts:
save(performace_svm, file="../output/performance_cv.RData")
}
tm_train=NA
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
best_gamma<-performace_svm$gamma[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,
gamma=best_gamma,cost = best_cost))
} else {
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y))
}
run.cv=F
if (run.cv){
##Get the parameters with the best results:
best_cost<-performace_svm$cost[which.min(performace_svm$error)]
best_gamma<-performace_svm$gamma[which.min(performace_svm$error)]
##Train the model:
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y,
gamma=best_gamma,cost = best_cost))
} else {
tm_train <- system.time(fit_train <- train_svm (Train.x,Train.y))
}
save(fit_train, file="../output/fit_train.RData")
##Get the training accuracy:
pre_train<-predict(fit_train,Train.x)
mean(pre_train==Train.y)
##Get the test accuracy:
pre_test<-predict(fit_train,Test.x)
mean(pre_test==Test.y)
knitr::opts_chunk$set(echo = TRUE)
packages.used=c("e1071","EBImage","ggplot2")
packages.needed=setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
if(length(packages.needed)>0){
install.packages(packages.needed, dependencies = TRUE)
}
library(EBImage)
library(e1071)
library(ggplot2)
##here is where all the given 2000 pictures are put
experiment_dir <- "../data/training_data/raw_images/"
##here is where all the independent testing set are put (it will be given in class)
#testing_dir <- "../data/testing_data/"
set_seed=FALSE         #use set.seed() whenever randomization needed
run.cv=TRUE           # run cross-validation on the training set
K <- 5                 # number of CV folds
train_proportion=0.75  # Porportion of the data that used for training the model
new.feature.train =TRUE      #process features for gievn training set
new.feature.test=FALSE       # process features for independent testing set
run.test=FALSE         # run evaluation on an independent test set
y<- read.table("../data/training_data/labels.csv", header=T)
y<-as.factor(t(y))
n<-length(y)
source("../lib/extract_feature.R")
tm_feature_train <- NA
if(new.feature.train){
tm_feature_train<- system.time(X <- extract_feature(experiment_dir,                                                 data_name="Newfeature_train", export=T))
#X is 2000*1024
} else {
X <-t(read.csv("../data/training_data/sift_features/sift_features.csv",header=T))
#X is 2000*5000
}
experiment_dir
getwd()
