---
title: "RFs_main"
author: "Chenyun Zhu"
date: "3/21/2017"
output: html_document
---
```{r}
 if(!require("randomForest")){
   install.packages("randomForest")
 }
 if(!require("ROCR")){
   install.packages("ROCR")
 }
 if(!require("knitr")){
   install.packages("knitr")
 }

library(knitr)
library(randomForest)
library(ROCR)

```

#### Step 0: specify directories.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
opts_knit$set(root.dir = "~/Desktop/GR5243/spr2017-proj3-grp1/lib/Features/GIST")
```


#### Step 1: set up controls for evaluation experiments.
```{r}
proportion = 0.75 # training set proportion
seed = 0 # set seed
```

#### Step 2: import training data
Randomly split the data into test and training set (75% training set and 25% test set)
```{r}
#setwd("~/Desktop/GR5243/spr2017-proj3-grp1")
#label.train <- read.csv("./data/sift_labels.csv")
#features <- read.csv("./data/sift_features.csv")

label.train <- as.factor(as.vector(c(rep(0,1000), rep(1,1000))))
features.1 <- read.csv("chickf.csv", header = F) ## label 0
features.2 <- read.csv("dogf.csv", header = F) ## label 1

features <- rbind(features.1, features.2)

n <- dim(features)[1]
set.seed(seed)
index <- sample(n, n*proportion)

x.train <- features[index,]
y.train <- label.train[index]

x.test <- features[-index,]
y.test <- label.train[-index]
```

#### Step 3: Find the number of trees where the out of bag error rate stabilizes and reach minimum.
```{r}
rf200 <- randomForest(x.train, y.train, ntree = 200)
rf300 <- randomForest(x.train, y.train, ntree = 300)
rf400 <- randomForest(x.train, y.train, ntree = 400)
rf500 <- randomForest(x.train, y.train, ntree = 500)
rf600 <- randomForest(x.train, y.train, ntree = 600)
rf700 <- randomForest(x.train, y.train, ntree = 700)
rf800 <- randomForest(x.train, y.train, ntree = 800)

# When ntree = 600 and 700, the OOB error rate stabilizes and reach minimum.

```
#### Step 4: Find the optimal number of variables selected at each split 
```{r}
set.seed(0)
rf <-randomForest(x.train, y.train, mtry=best.m, importance = T, ntree=600)
## If you want to see the important variables, run the following
## importance(rf)
## varImpPlot(rf)
```

#### Step 5: Find the optimal number of variables selected at each split
```{r}
mtry <- tuneRF(x.train, y.train, ntreeTry=600, stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
```

#### Step 6: Prediction
```{r}
pred1 <- predict(rf)
mean(pred1 != y.train)
```

#### Evaluation
```{r}
# Evaluate the performance of the random forest for classification.
pred2=predict(rf,type = "prob")

# prediction is ROCR function
perf = prediction(pred2[,2], y.train)

# performance in terms of true and false positive rates
# AUC
auc = performance(perf, "auc")

# True Positive and Negative Rate
pred3 = performance(perf, "tpr","fpr")

# Plot the ROC curve
plot(pred3,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

```

