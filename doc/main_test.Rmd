---
title: "Project 3 - Test Main Script"
author: "Marie Schiltz"
date: "March 10, 2017"
output:
  pdf_document: default
  html_document: default
---

#### Step 1: Load Packages.

```{r}
library("gbm")
```

#### Step 2: Specify Directories.

Provide directory for SIFT features
```{r}
sift_dir <- "../../data/sift_features/"

```


Upload SIFT features
```{r}
sift_dir <- "/Users/Marie/Documents/Columbia/Applied Data Science/Project 3/data/sift_features/"
sift <- read.csv(paste0(sift_dir, "sift_features.csv"))
```

Upload Labels
```{r}
labels_dir <- "/Users/Marie/Documents/Columbia/Applied Data Science/Project 3/data/"
labels <- read.csv(paste0(labels_dir, "labels.csv"))
```




#### Step 3: Separate Train and Test Dataset
Full Dataset
```{r}
# Invert columns and rows
sift <- data.table::transpose(sift)

# Put labels and features in the same table
names(labels) = c("labels")
df <- cbind(labels, sift)
```

Training and Test Set - Whole Dataset
Then we can use only the train set to train the models
```{r}
per_train <- 0.75 # percentage of training data
smp_size <- floor(per_train * nrow(df)) # size of the sample
set.seed(618)
smp_ind <- sample(seq_len(nrow(df)), size = smp_size)
train <- df[smp_ind, ]
test <- df[-smp_ind, ]
```

Take a subset of the training set
```{r}
subset_size <- 200
set.seed(123)
subset_smp_ind <- sample(seq_len(nrow(train)), size = subset_size)
train.subset <- train[subset_smp_ind,]
```



## Baseline: GBM

Fit Boosted Decision Stumps
Decision Stump: One level decision tree
```{r}
boost.fit <- gbm(labels~., data=train.subset,
              distribution="bernoulli",
              n.trees=10000,
              interaction.depth=1,
              shrinkage=0.001)
boost.fit
summary(boost.fit)
```


```{r}
boost.fit$train.error[10000]
```

Number of trees
```{r}
gbm.perf(boost.fit, method="OOB")
```

2000 trees seem to be enough

Cross-Validation
Choose Shrinkage Parameter

Test CV to see if it doesn't burn my computer
```{r}
boost.fit <- gbm(labels~., data=train.subset,
              distribution="bernoulli",
              n.trees=2000,
              interaction.depth=1,
              shrinkage=0.001,
              cv.folds=8)

boost.fit$cv.error[2000]
```

CV on shrinkage
```{r}
boost.err <- data.frame(matrix(nrow=8000,ncol=3))

i = 0
for (shr in c(0.1, 0.01, 0.001)){
  i = i+1
  boost.fit.temp <- gbm(labels~., data=train.subset,
              distribution="bernoulli",
              n.trees=8000,
              interaction.depth=1,
              shrinkage=shr,
              cv.folds=5)
  boost.err[,i] <- boost.fit.temp$cv.error
}
```

```{r}
boost.err.c <- boost.err
boost.err.c$index <- seq_len(8000)
boost.err.c <- gather(boost.err.c, condition, error, X1:X3, factor_key=FALSE)

g <- ggplot(boost.err.c, aes(index, error, colour=condition)) +
  geom_line() +
  ylab("CV Error Rate") +
  xlab("Number of trees") + 
  labs(title="Baseline - GBM - Depth of 1") +
  scale_color_discrete(name="Shrinkage", labels=c("0.1","0.01","0.001"))
g
```

```{r}
boost.fit <- gbm(labels~., data=train.subset,
              distribution="bernoulli",
              n.trees=15000,
              interaction.depth=1,
              shrinkage=0.001,
              cv.folds=5)

boost.fit$cv.error[15000]
bosst.fit.err <- boost.fit$cv.error
bosst.fit.err <- cbind(seq_len(15000), bosst.fit.err)
```

Now that we have chosen the parameters
Shrinkage: 0.001
Nb of trees: 13000
Fit to the whole model & store the time
```{r}
boost.fit.time <- 
  system.time(boost.fit <- gbm(labels~., data=train,
              distribution="bernoulli",
              n.trees=13000,
              interaction.depth=1,
              shrinkage=0.001))
```

Estimate the generalization error using the test set
```{r}
boost.probs <- predict(boost.fit, test, n.trees=10000, type="response")
boost.pred=rep(0,nrow(test))
boost.pred[boost.probs >.5]=1
table(boost.pred, test$labels)
1 - mean(boost.pred==test$labels)
```
Generalization error Using Gbm: 0.268


## GBM - With a depth different of one

```{r}

```

## Logistic Regression

Subdivise Training set into Training and Validation
```{r}

```

```{r}
glm.fit=glm(labels~., data=train_boston ,family=binomial)
summary(glm.fit)
```

## SVM Linear

```{r}
svmfit <- svm(labels~., data=train, kernel="linear", cost=10, scale=FALSE)
```

```{r}
tune.svm.linear <- tune(svm, labels~. , data=train, kernel="linear", 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)), scale=F)
summary(tune.svm.linear)
```

Remove constant features - using the training set - not using the tesst set
```{r}
cat("n## Removing the constants features.n")
train.rm <- train
test.rm <- test
for (f in names(train)) {
  if (length(unique(train[[f]])) == 1) {
    cat(f, "is constant in train. We delete it.n")
    train.rm[[f]] <- NULL
    test.rm[[f]] <- NULL
  }
}
```



```{r}
tune.svm.linear <- tune(svm, labels~. , data=train.pca, kernel="linear", 
                 ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.svm.linear)
```
Best Performance: 

```{r}
pca.fit <- prcomp(sift[, 1:5000],
                 center = TRUE,
                 scale. = TRUE)

train.pca <- predict(pca.fit, 
                  newdata=train[, 2:5001])
train.pca <- as.data.frame(train.pca[, 1:100])
train.pca <- cbind(train[, 1], train.pca)
names(train.pca)[1]="labels"
```

```{r}
tcon<-tune.control(sampling="cross",cross=5)
bsvm <- best.svm(labels~.,data=train,kernel="linear", 
                     gamma = 2^(-2:2), cost = 2^(0:4),
                     tunecontrol=tcon)
```

## SVM Radial
```{r}
tune.svm.radial <- tune(svm, Class~., data=train.cancer, kernel="radial", 
                        ranges=list(cost=c(0.1,1,10,100,1000), 
                                    gamma=c(0.5,1,2,3,4)))
summary(tune.svm.radial)
```
Best Performance: 

## xgboost

## randomForest

```{r}
m = floor(sqrt(5000))
rf.fit <- randomForest(labels~., data=df.subset, mtry=m, importance =TRUE,
                         ntree=3000)
```

Pick the best mtry
```{r}
rf.err <- data.frame(matrix(nrow=40,ncol=4))

for (i in 20:70){
  rf.fit <- randomForest(med_crim~., data=df_boston.f, mtry=i, importance =TRUE,
                         ntree=10000)
  rf.err[i,1] <- i
  rf.err[i,2] <- rf.fit$err.rate[nrow(rf.fit$err.rate),1]
}

```


## PCA

```{r}
library(stats)
pca.fit <- prcomp(sift[, 1:5000],
                 center = TRUE,
                 scale. = TRUE)

plot(df.pca, type = "l")
summary(df.pca)

df.pca <- predict(pca.fit, 
                  newdata=sift[, 1:5000])
df.pca <- as.data.frame(df.pca[, 1:50])
df.pca <- cbind(labels, df.pca)
```

## LDA

```{r}
per_train <- 0.75 # percentage of training data
smp_size_all <- floor(per_train * nrow(df.pca)) # size of the sample
set.seed(12)
smp_ind_all <- sample(seq_len(nrow(df.pca)), size = smp_size_all)
train.all.pca <- df.pca[smp_ind_all, ]
test.all.pca <- df.pca[-smp_ind_all, ]
```

```{r}
library(MASS)
lda.fit<-lda(labels~., data=train.all.pca)
# not working because of constents features, retry after pca
lda.pred  <- predict(lda.fit, subset(test.all.pca, select=-labels))
1-mean(lda.pred$class==test.all.pca$labels)
#32% Error rate
```

```{r}
qda.fit<-qda(labels~., data=train.all.pca)
qda.pred  <- predict(qda.fit, subset(test.all.pca, select=-labels))
1-mean(qda.pred$class==test.all.pca$labels)
```

